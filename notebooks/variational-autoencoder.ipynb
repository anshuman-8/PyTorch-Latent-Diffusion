{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from decoder import VAE_AttentionBlock, VAE_ResidualBlock\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Sequential): # sequence of modules to reduce the dimensionality of the input\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            VAE_ResidualBlock(128, 128), # maybe to increase the depth of the network\n",
    "            VAE_ResidualBlock(128, 128), # maybe to increase the number of parameters\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),  # height and width are halved\n",
    "\n",
    "            VAE_ResidualBlock(128, 256),\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),  # height and width are halved\n",
    "\n",
    "            VAE_ResidualBlock(256, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0),  # height and width are halved\n",
    "\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            VAE_AttentionBlock(512),\n",
    "\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "            # nn.GroupNorm(32, 512), # 32 is the number of groups(BatchNorm2d(512) is the same)\n",
    "            nn.BatchNorm2d(512),\n",
    "\n",
    "            nn.SiLU(), # nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1), \n",
    "            nn.Conv2d(8, 8, kernel_size=1, padding=0), # linear transformation to reduce the dimensionality of the input\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor, noice:torch.Tensor)-> torch.Tensor:\n",
    "        # x: (batch_size, 3, 512, 512)\n",
    "        # noise: (batch_size, 3, 512, 512)\n",
    "        for modules in self:\n",
    "            print(modules)\n",
    "            if isinstance(modules, VAE_AttentionBlock):\n",
    "                x = modules(x, noice)\n",
    "            else:\n",
    "                x = modules(x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
